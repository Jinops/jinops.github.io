---
layout: post
title: "오픈소스 모델과 로봇 개발 프로젝트"
subtitle: 컨베이어 벨트 물체 정렬 로봇
date: 2024-01-16 13:00:00 +0900
background: 
categories: [회고, 프로젝트]
---

## Repositories
https://github.com/Jinops/conveyor-alignment-robot

https://github.com/Jinops/yolov5_obb

## 개요
학교 졸업 프로젝트의 일환으로 컨베이어 벨트 물체 정렬 로봇을 개발하였다. 간단하게 설명하면, 컨베이어 벨트 위의 틀어져있는 물체를, 로봇 팔을 통해 정방형으로 정렬해주는 것이다.

4인 팀으로 구성되어 진행한 이 프로젝트는, 여러가지 주제 선정 과정 중 *공항 baggage claim(입국 시 수화물을 찾는 공간)에 캐리어가 한 번에 많이나올 경우, 공간 최적화를 위해 직원이 직접 가로로 눕히거나 빼던* 필자의 경험을 토대로 고도화하고자 하였다. 이후  공장 내 물류의 정렬을 통한 물류 정리 또는 QA 일관성 유지에 도움을 주고자 하는 목적으로 길을 틀었다.

## 방법론 선정
핵심적으로 필요한 기능 중 하나는, 카메라를 통해 촬영된 컨베이어 벨트 위 물체가 틀어져있는지를 인식하는 것이었다. 이를 위해 segmentation과 object detection를 대해 가장 먼저 알아봤다. 다만 두 방법 모두 회전량을 인식하는 것은 아니였다. 방법론을 찾아보는 중, YOLOv5 모델을 커스텀한 오픈소스 모델 YOLOv5_obb(Oriented Object Detection)을 검토하였고 최종적으로 선정하였다.

> YOLOv5_obb는 bounding box를 만들기 위한 점을 4개(기존 2개)를 사용하여, 물체가 회전되어 있을 경우 그에 맞게 bounding box도 회전이 된다.

## 분업
4명의 팀원 중 본인은 인프라 설계 및 구축, 통신, 로봇 프로그래밍을 담당했다. 그 외 팀원은 각각 Model fitting, 로봇팔 이동거리 계산(이하 알고리즘), 데이터셋 생성을 담당했다.

## 아키텍쳐 설계
![아키텍쳐](https://github.com/Jinops/conveyor-alignment-robot/raw/main/flow_diagram.png)

나눠진 개발 범위를 토대로, 크게 Model Server, Main Computer, Robot으로 구분하였다. 
- **Main Computer**에서는 이미지 촬영 및 전처리, 알고리즘을 담당한다. 
- **Model Server**에서는 말 그대로 pre-train된 모델을 사용할 수 있는 서버를 구축했다. 
- **Robot**은 Main Computer로부터 전달받은 이동거리만큼 로봇팔을 움직인다. 

> Main Computer와 Model Server를 꼭 나눌 필요는 없지만, 1.최대한 현업에 유사할 수 있도록 하기 위해 2.개발 환경의 차이로 구분하였다.

이와 같이 구분한 후, 필자는 Main Computer와 Robot 개발에 집중을 하였다.

## 로봇 개발

### 로봇 선정
처음 사용하려 했던 로봇은 그리퍼가 탑제된 Jetson Nano 기반의 로봇이었다. ROS 환경에서 개발을 하려 했던 초기 계획은 개발 환경의 문제(필자가 사용하는 M1 Mac에는 ROS1을 지원하는 Ubuntu 18 설치 불가)를 해결한 직후, 지도 교수님의 제안 및 개발 범위에 비해 과도한 노력이 필요한 것 같다 판단해 변경하였다.

최종적으로 Elephant Robotics사의 MyCobot280이라는 6축로봇을 사용하였다. 특별히 선정했다기보단, 연구실에서 대여할 수 있는 로봇을 사용했다.

### 어려움
로봇 개발이 처음이기 때문에, 초기 설정에 버벅임이 있었다. 또한 
documentation에 deprecated된 내용이 있었고, 최신 내용은 중국어로 작성되어 있기도 했다. 그래도 셋팅을 완료한 후부터 크게 어려운 점은 없었던 것 같다. 

### 아두이노 보드
보드는 Mega 2560를 사용했다. 처음에는 Rx/Tx 포트가 각 1개 씩 있는 Uno를 사용했는데, 로봇 본체와의 연결에 이미 해당 포트를 사용하고 있는 상태에서 Main PC와의 시리얼 통신을 위한 포트가 더 필요해 변경하였다.

### 구현
Main PC로부터 전달받은 거리(0~100) 값을 기준으로, 로봇팔을 움직이고자 하였다. 따라서 
아두이노에서 사용할 수 있는 여러 API에 대해 테스트를 거친 뒤, 크게 두 가지 기능을 구현하였다.
1. Calibration: 0지점과 100지점을 초기화한다. 로봇팔의 토크를 풀고, 사람이 6축의 포지션을 직접 움직여 두 위치를 지정하도록 하였다.
2. Move: 전달받은 거리 값으로부터 이동할 지점을 계산하여 움직인다. 물체 끼임 방지를 위해 일정 시간 후 높이를 들어준다.

## 모델 개발환경과 서버
### 개발환경 구축의 어려움 
가장 시간을 많이 허비한 부분이지 않을까 싶다. 팀원 중 누구도 오픈소스 기반의 커스텀 모델은 사용해본 적이 없었다. 파인튜닝을 하고 모델 성능을 개선해보기 이전에, 모델을 실행조차 할 수 없었다. 주된 원인은 개발환경의 차이(특히 os)였다.

모델을 fork해와서, 원본 레포지토리의 issues를 참고하며 어려가지를 시도해보았다. 이또한 대부분이 중국어여서 어려움이 있었다. 우리에게 문제가 있었던 부분을 모두 해결하고, 누군가는 도움이 될 수 있도록 영문 readme와 troubleshooting한 부분을 짧게 정리하였다. 
https://github.com/Jinops/yolov5_obb

계획했던 것보단 일정이 밀렸지만, 이 부분을 해결한 뒤 모델을 담당했던 팀원도 답답함을 해소하고 역할에 충실할 수 있었다.

### 서버
개인 PC에 cuda 등 개발환경을 구축하고, Fast API로 서버를 구축하였다. 이미지를 전달받으면, 모델의 결과를 return해주는 간단한 post api를 만들고, 모델이 우리가 원하는 형태의 결과를 보여줄 수 있도록 옵션(파라미터)를 수정하였다.

## 메인 코드
Main Computer에서 실행되는 메인 코드는, 이미지 촬영 및 전처리, 통신을 담당한다. 결과로 받은 bounding box 정보 등을 필터링하고, 알고리즘을 통해 물체의 회전량 및 로봇팔 이동 거리를 계산하여 로봇에게 전송하도록 하였다. 시각화는 기획에 없었지만, 괜찮은 결과물을 보여주고 싶어 간단하게 구현하였다. 덕분에 알고리즘 테스트에서도 용이하게 사용할 수 있었다.
<br/><br/>
최종 실험 결과는 Github과 [Youtube](https://youtu.be/UYmC-FVZ9Nw?si=A0M36d8f3CrFivbh)에 게시하였다. 

## 프로젝트 성과
무사히 통과가 되어 졸업요건을 충족할 수 있게 되었다. 또한 같은 결과물로 참여한 교내 캡스톤디자인 경진대회에 1차 통과하여, 1월 25일에 최종 발표 평가를 앞두고 있다. 팀플이나 발표를 좋아해서 프로젝트 자체도 재미있게 했는데, 학교에서의 마지막 발표를 모든 학과를 대상으로 한 경진대회에서 할 수 있어서 기대하며 준비하고 있다. 

## 회고
산업공학도이자 스마트팩토리에 관심이 있었던 사람으로서, 이번 프로젝트의 구성, 개발 범위 등 전체적으로 만족스러웠었다. 아키텍쳐를 설계하고, 로봇을 작동하며, 모델을 튜닝해가는 과정까지 더해지니 이전 학부 프로젝트들에 비해 완성도 있는 결과물이 나온 것 같다.

개발 관련된 부분은 대부분 필자가 담당하였고 책임이 있었다. 각자의 이해도에 따라 배분하였기 때문에 거부감은 없었고, 스스로 알고 있던 지식을 리마인드하고 새로운 공부 또한 할 수 있어서 좋았다. 그러나 깊이 있는 학습을 하기에는 한계가 있었다. 가령 로봇 개발을 ROS를 통해 수행했다면, 시간은 더 오래걸릴지라도 새로운 개발분야를 시작할 수 있었을 것이다. 

또한 서버도 알고 있는 Fast API를 이용해 개인 PC에 구축을 하였지만, Flask을 이용하고 클라우드에 호스팅을 하면 더 좋았을걸 생각이 든다. 

전반적으로 제한된 시간 내에 구현을 하기 위해 간단한 방법을 찾으므로써 발생하는 아쉬움들인 것 같다.
